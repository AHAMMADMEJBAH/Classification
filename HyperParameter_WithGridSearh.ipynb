{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What is GridSearchCV?\n",
    "GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What Is Grid Search?\n",
    "Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. It is an exhaustive search that is performed on a the specific parameter values of a model. The model is also known as an estimator. Grid search exercise can save us time, effort and resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_csv('../../../DataSets/titanic_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 712 entries, 0 to 711\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    712 non-null    int64  \n",
      " 1   Pclass      712 non-null    int64  \n",
      " 2   Sex         712 non-null    int64  \n",
      " 3   Age         712 non-null    float64\n",
      " 4   SibSp       712 non-null    int64  \n",
      " 5   Parch       712 non-null    int64  \n",
      " 6   Fare        712 non-null    float64\n",
      " 7   Embarked_C  712 non-null    int64  \n",
      " 8   Embarked_Q  712 non-null    int64  \n",
      " 9   Embarked_S  712 non-null    int64  \n",
      "dtypes: float64(2), int64(8)\n",
      "memory usage: 55.8 KB\n"
     ]
    }
   ],
   "source": [
    "datasets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datasets.drop('Survived', axis = 1)\n",
    "y = datasets['Survived']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_classification(y_test, y_pred):\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred, normalize=True)\n",
    "    accuracy_number = accuracy_score(y_test, y_pred, normalize=False)\n",
    "    r_score = r2_score(y_test, y_pred)\n",
    "    recall_scores = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    \n",
    "    print('Test Data:\\t', len(y_test))\n",
    "    print('Accuracy:\\t', accuracy)\n",
    "    print('r2_score:\\t', r_score)\n",
    "    print('recall score:\\t', recall_scores)\n",
    "    print('Precision Score:\\t',precision)\n",
    "    print('Accuracy Number:\\t', accuracy_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'max_depth': [2, 4, 5, 7, 9, 10]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'max_depth':[2, 4, 5, 7, 9, 10]}\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(),parameters, cv = 3, return_train_score=True)\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter:\t {'max_depth': 2}\n",
      "\n",
      "Mean Test score is :\t 0.7907917943005662\n",
      "\n",
      "Rank:\t 3\n",
      "Parameter:\t {'max_depth': 4}\n",
      "\n",
      "Mean Test score is :\t 0.7942727188341223\n",
      "\n",
      "Rank:\t 1\n",
      "Parameter:\t {'max_depth': 5}\n",
      "\n",
      "Mean Test score is :\t 0.785500789009561\n",
      "\n",
      "Rank:\t 4\n",
      "Parameter:\t {'max_depth': 7}\n",
      "\n",
      "Mean Test score is :\t 0.7908010767659891\n",
      "\n",
      "Rank:\t 2\n",
      "Parameter:\t {'max_depth': 9}\n",
      "\n",
      "Mean Test score is :\t 0.7820012995451592\n",
      "\n",
      "Rank:\t 6\n",
      "Parameter:\t {'max_depth': 10}\n",
      "\n",
      "Mean Test score is :\t 0.7837556855100715\n",
      "\n",
      "Rank:\t 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print('Parameter:\\t', grid_search.cv_results_['params'][i])\n",
    "    print()\n",
    "    print('Mean Test score is :\\t',grid_search.cv_results_['mean_test_score'][i])\n",
    "    print()\n",
    "    print('Rank:\\t', grid_search.cv_results_['rank_test_score'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = DecisionTreeClassifier(max_depth=grid_search.best_params_['max_depth']).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decision_tree_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data:\t 143\n",
      "Accuracy:\t 0.7692307692307693\n",
      "r2_score:\t 0.018102372034956238\n",
      "recall score:\t 0.6666666666666666\n",
      "Precision Score:\t 0.7058823529411765\n",
      "Accuracy Number:\t 110\n"
     ]
    }
   ],
   "source": [
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'penalty':['l1','l2'],\n",
    "             'C':[0.1, 0.4, 0.8, 1, 2, 5]}\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='liblinear'), parameters, cv = 3, return_train_score=True)\n",
    "grid_search.fit(x_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\t {'C': 0.1, 'penalty': 'l1'}\n",
      "Mean test score:\t 0.7767010117887311\n",
      "Rank:\t 12\n",
      "Parameters:\t {'C': 0.1, 'penalty': 'l2'}\n",
      "Mean test score:\t 0.7890374083356538\n",
      "Rank:\t 10\n",
      "Parameters:\t {'C': 0.4, 'penalty': 'l1'}\n",
      "Mean test score:\t 0.7960456697298802\n",
      "Rank:\t 7\n",
      "Parameters:\t {'C': 0.4, 'penalty': 'l2'}\n",
      "Mean test score:\t 0.7837649679754942\n",
      "Rank:\t 11\n",
      "Parameters:\t {'C': 0.8, 'penalty': 'l1'}\n",
      "Mean test score:\t 0.7978186206256382\n",
      "Rank:\t 6\n",
      "Parameters:\t {'C': 0.8, 'penalty': 'l2'}\n",
      "Mean test score:\t 0.7925461802654784\n",
      "Rank:\t 8\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print('Parameters:\\t', grid_search.cv_results_['params'][i])\n",
    "    print('Mean test score:\\t', grid_search.cv_results_['mean_test_score'][i])\n",
    "    print('Rank:\\t',grid_search.cv_results_['rank_test_score'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(solver='liblinear',penalty = grid_search.best_params_['penalty'], C = grid_search.best_params_['C'])\n",
    "final_model = logistic_model.fit(x_train, y_train)\n",
    "y_pred = final_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data:\t 143\n",
      "Accuracy:\t 0.7972027972027972\n",
      "r2_score:\t 0.1371202663337494\n",
      "recall score:\t 0.7037037037037037\n",
      "Precision Score:\t 0.7450980392156863\n",
      "Accuracy Number:\t 114\n"
     ]
    }
   ],
   "source": [
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
